[
  {
    "objectID": "posts/r_practice/index.html",
    "href": "posts/r_practice/index.html",
    "title": "R Practice",
    "section": "",
    "text": "Through the course “Becoming Fluent in Data and Beyond,” I completed five assignments focused on honing my R skills rather than performing in-depth analysis. These assignments provided hands-on experience with various data types, including time-series, geospatial, panel data, and webscrapping. Here I am sharing some of my favourite plots and analysis showcasing different skills.\n\n\nThis practice provided valuable experience in working with time-series data, converting text data types into time format, and identifying implausible recordings.\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load data\nViaRun_10_05_2023 &lt;- readxl::read_xlsx(\"ViaRun_10_05_2023.xlsx\")\n\n# Convert time string to POSIXct object and extract minutes and seconds (adjusted for the mistake)\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  mutate(time = as.POSIXct(Zeit, format = \"%Y-%m-%d %H:%M:%S\"),\n         Minutes = hour(time),\n         Secs = minute(time))\n\n# Convert Minutes and Secs to a time object\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;% \n  mutate(Time_Fixed = as.POSIXct(paste0(\"00:\", Minutes, \":\", Secs), format = \"%H:%M:%S\"))\n\n# Filter out rows with NA in Time_Fixed column\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  filter(!is.na(Time_Fixed))\n\n# Filter Implausible Records\nimplausible_recording &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &lt;= as.POSIXct(\"00:13:07\", format = \"%H:%M:%S\"))\n\n# Filter everything below 10 minutes\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &gt;= as.POSIXct(\"00:10:00\", format = \"%H:%M:%S\"))\n\n\n\n\nCode\nViaRun_10_05_2023 %&gt;%\n  filter(Gender != \"divers\") %&gt;%\n  group_by(Gender) %&gt;%\n  ggplot(aes(x = Time_Fixed, fill = Gender)) +\n    geom_density(alpha = 0.5, color = \"NA\") +\n    labs(title = \"Density Plot of Time by Gender\",\n       x = \"Time\", y = \"Density\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/r_practice/index.html#various-r-assignments-for-practice",
    "href": "posts/r_practice/index.html#various-r-assignments-for-practice",
    "title": "R Practice",
    "section": "",
    "text": "Through the course “Becoming Fluent in Data and Beyond,” I completed five assignments focused on honing my R skills rather than performing in-depth analysis. These assignments provided hands-on experience with various data types, including time-series, geospatial, panel data, and webscrapping. Here I am sharing some of my favourite plots and analysis showcasing different skills.\n\n\nThis practice provided valuable experience in working with time-series data, converting text data types into time format, and identifying implausible recordings.\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load data\nViaRun_10_05_2023 &lt;- readxl::read_xlsx(\"ViaRun_10_05_2023.xlsx\")\n\n# Convert time string to POSIXct object and extract minutes and seconds (adjusted for the mistake)\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  mutate(time = as.POSIXct(Zeit, format = \"%Y-%m-%d %H:%M:%S\"),\n         Minutes = hour(time),\n         Secs = minute(time))\n\n# Convert Minutes and Secs to a time object\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;% \n  mutate(Time_Fixed = as.POSIXct(paste0(\"00:\", Minutes, \":\", Secs), format = \"%H:%M:%S\"))\n\n# Filter out rows with NA in Time_Fixed column\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  filter(!is.na(Time_Fixed))\n\n# Filter Implausible Records\nimplausible_recording &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &lt;= as.POSIXct(\"00:13:07\", format = \"%H:%M:%S\"))\n\n# Filter everything below 10 minutes\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &gt;= as.POSIXct(\"00:10:00\", format = \"%H:%M:%S\"))\n\n\n\n\nCode\nViaRun_10_05_2023 %&gt;%\n  filter(Gender != \"divers\") %&gt;%\n  group_by(Gender) %&gt;%\n  ggplot(aes(x = Time_Fixed, fill = Gender)) +\n    geom_density(alpha = 0.5, color = \"NA\") +\n    labs(title = \"Density Plot of Time by Gender\",\n       x = \"Time\", y = \"Density\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/r_practice/index.html#analysis-of-pokemon-players-in-germany",
    "href": "posts/r_practice/index.html#analysis-of-pokemon-players-in-germany",
    "title": "R Practice",
    "section": "Analysis of Pokemon players in Germany",
    "text": "Analysis of Pokemon players in Germany\nThis was a good practice to work with geospatial data and creating interactive map. Pokemon GO gameplay data was used to plot players’ behavior in Germany including frequency and movement.\n\n\nCode\n# Load libraries\nlibrary(leaflet)\nlibrary(leaflet.extras)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# Load Data\nGameData &lt;- read_delim(\"GameplayLocationHistory.tsv\")\n\n# Rename the columns\nGameData &lt;- GameData %&gt;%\n  rename(\n    lat = `Latitude of location reported by game`,\n    lon = `Longitude of location reported by game`)\n\n# Convert the GameData to a spatial object\ngeodata_sf &lt;- st_as_sf(GameData, coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# Load country boundaries using rnaturalearth\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Filter for Germany\ngermany_boundary &lt;- world %&gt;% \n                    filter(admin == \"Germany\")\n\n# Perform a spatial join to filter points within Germany\ngermany_points &lt;- st_join(geodata_sf, germany_boundary, join = st_within)\n\n\n# Extract lon and lat from geometry column\ngermany_points &lt;- germany_points %&gt;%\n  st_coordinates() %&gt;%\n  as.data.frame() %&gt;%\n  rename(\n    lon = X,\n    lat = Y) %&gt;%\n  bind_cols(germany_points %&gt;% select(-geometry))\n\n# extracting only needed columns\ngermany_points &lt;- germany_points %&gt;% \n  select(1:3)\n\n\n\n\nCode\nlibrary(viridis)\n\n# Calculating start, end date and max date\nstart_date &lt;- min(as.Date(germany_points$`Date and Time`, format = \"%Y-%m-%d\"), na.rm = TRUE)\nend_date &lt;- max(as.Date(germany_points$`Date and Time`, format = \"%Y-%m-%d\"), na.rm = TRUE)\nmax_days &lt;- as.numeric(difftime(end_date, start_date, units = \"days\"))\n\n# Calculating the number of days each location has been recorded\ngermany_points_map &lt;- germany_points %&gt;%\n  mutate(\n    `Date and Time` = as.Date(`Date and Time`),\n    days_recorded = as.numeric(difftime(`Date and Time`, start_date, units = \"days\")))\n\n# Define color palette with viridis\ncolor_palette &lt;- colorNumeric(\n  palette = viridis(7),\n  domain = c(0, 20, 40, 60, 80, 100, 120)\n)\n\n# Create the interactive map\nleaflet(germany_points_map) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(\n    ~lon, ~lat,\n    color = ~color_palette(days_recorded),\n    radius = 3,\n    fillOpacity = 0.8,\n    popup = ~paste(\"Days Recorded:\", days_recorded),\n    label = ~paste0(germany_points$`Date and Time`, \" (Days since start: \", days_recorded, \")\")\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = color_palette,\n    values = ~days_recorded,\n    title = paste(\"Days Since Start:\", \n                  paste(\"Start:\", format(start_date, \"%Y-%m-%d\")),\n                  paste(\"End:\", format(end_date, \"%Y-%m-%d\")),\n                  sep = \"&lt;br&gt;\"),\n    opacity = 1,\n    labFormat = labelFormat(transform = function(x) x)\n  ) %&gt;% \n  \n# Setting the view fixed to Berlin\nsetView(lng = 13.4050, lat = 52.5200, zoom = 6)"
  },
  {
    "objectID": "posts/r_practice/index.html#analysis-of-most-expensive-paintings-in-history",
    "href": "posts/r_practice/index.html#analysis-of-most-expensive-paintings-in-history",
    "title": "R Practice",
    "section": "Analysis of most expensive paintings in history",
    "text": "Analysis of most expensive paintings in history\nHere, I have scrapped a table from a Wikipedia page and cleaned and used the scrapepd to analyze relationships between variables.\n\n\nCode\nlibrary(rvest)\nlibrary(readr)\n\n# URL of the website to scrape\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_most_expensive_paintings#Progression_of_highest_prices_paid\"\n\npage &lt;- read_html(url)\n\ndf_wp &lt;- page %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE)\n\npaintings &lt;- data.frame(df_wp)\n\n\n\n\nCode\ndf &lt;- read_csv(\"paintings_df.csv\")\n\n# Selecting only needed columns\ndf &lt;- df %&gt;% \n  select(1, 3, 5, 6, 7)\n\n# Renaming columns required\ndf &lt;- df %&gt;% \n  rename(\n    Price = 1,\n    Date = 5\n  )\n\n# Cleaning Price column\ndf &lt;- df %&gt;%\n  mutate(Price_cleaned = Price) %&gt;%  \n  mutate(Price_cleaned = str_replace(Price_cleaned, \"~\", \"\")) %&gt;%\n  mutate(Price_cleaned = str_replace(Price_cleaned, \"\\\\+\", \"\")) %&gt;% \n  mutate(Price_cleaned = str_replace(Price_cleaned, \"\\\\$\", \"\")) %&gt;%\n  mutate(Price_cleaned = str_trim(Price_cleaned)) %&gt;%\n  mutate(Price_cleaned = as.numeric(Price_cleaned))\n\n# taking earleir year for ranged years\ndf &lt;- df %&gt;%\n  mutate(Year_cleaned = case_when(\n    # Handle years with shorthand end dates (e.g., 1904-07)\n    str_detect(Year, \"\\\\d{4}-\\\\d{2}\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # If there's a dash or slash, take the first 4-digit number\n    str_detect(Year, \"-|/\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # If it starts with \"c.\", take the 4-digit number\n    str_starts(Year, \"c\\\\.\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # Otherwise, extract any 4-digit number\n    TRUE ~ as.numeric(str_extract(Year, \"\\\\d{4}\"))\n  ))\n\n# filtering out for final cleaned df\ndf &lt;- df %&gt;%\n  select(-1, -4, -5)\n\n\nSince the Wikipedia page did not provide the artist’s age at the time of painting or at death. I merged another table containing this information to analyze the correlation between the price of the paintings and both the artists’ ages at death and their ages at the time of painting.\n\n\nCode\nplot3 &lt;- df %&gt;%\n  filter(Price_cleaned &gt; 150)\n\n# built based on previous table, containing data above 150m USD\ndf3 &lt;- read_csv(\"above_150.csv\")\n\nplot3$age_at_painting &lt;- df3$Age.at.Painting\nplot3$age_at_death &lt;- df3$Age.at.Death\n\nggplot(plot3, aes(x = age_at_painting, y = Price_cleaned)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.8, se = FALSE, color = \"blue\") +\n  scale_x_continuous(breaks = seq(0, max(plot3$age_at_death), by = 20)) + \n  theme_minimal() +\n  labs(\n    title = \"List of Most Expensive Paintings\",\n    subtitle = \"Paintings over 150 million USD\",\n    x = \"Age at Painting\",\n    y = \"Adjusted Auction Price in Millions USD\"\n  )"
  },
  {
    "objectID": "posts/health_commuting/index.html",
    "href": "posts/health_commuting/index.html",
    "title": "Health and Commute",
    "section": "",
    "text": "The study builds on previous study by Goerke and Lorenz (2017) and investigates the potential correlation between sickness absence and commute distance using a fixed effects model on SOEP data from 2015 to 2019. While longer commutes are associated with a slight increase in sick days (2.7 additional days for every 17 km), this effect is statistically insignificant. Age is positively correlated with both more sick days (7.4 additional days for every 11 years older) and poorer self-reported health, with a stronger impact on health status. Higher income is associated with fewer sick days and better self-reported health. Full-time employment is associated with a significant increase in sick days (1.49 more days). Additionally, there is a noticeable trend of rising sick days over time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Udval Oyunsaikhan",
    "section": "",
    "text": "Hi! I’m Udval. My passion for data analysis was ignited during my time at Meta, where I worked extensively with large datasets. Currently, I am pursuing an MSc in International Business Administration with a specialization in Data Science at the University of Viadrina. This program is further developing my technical expertise and equipping me to drive data-driven enhancements in products, strategies, and operations. My interests lie in urban green mobility, transport data analysis, and trust and safety, with a particular focus on financial fraud and scams.\nFeel free to explore my work from my current studies and get in touch if you have any questions!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thank you for taking the time and showing curiosity to check out my portfolio. Here, you will find the projects I have completed since starting my study in April 2024. I will continue to update this portfolio with new projects. I am also open to collaborating on new projects—if you have a dataset and would like insights, feel free to reach out. Throughout my studies, I have different focus areas each semester:\nSemester 1: R, PowerBI, SQL, and Data Analysis\nSemester 2: Python, Tableau, ML, Statistics, & Business Analytics\nSemester 3: Marketing Analytics, A/B Testing, AI, & ML in Finance\nSemester 4: Currently, I am on the hunt for a research idea\n\nOutside this portfolio\nI hold a MSc in Sustainable Transport Degree from University of Leeds. I have published five articles, done podcast interviews and created and presented a video on public space at the UN Ministerial Summit in 2018.\nI love cooking and eating good food, so during COVID I started my own channel for cooking.\nFacebook | Youtube"
  },
  {
    "objectID": "posts/certificate/index.html",
    "href": "posts/certificate/index.html",
    "title": "Certification Exam",
    "section": "",
    "text": "As part of the certified data analyst exam by DataCamp, I completed a two-hour exam testing my R and SQL skills, followed by a four-hour practical exam where I analyzed sales data from a stationary shop.\nThe goal of the analysis was to determine the best-performing marketing method following the launch of new products. Additionally, I identified high-paying and loyal customers to prioritize them in the marketing strategy for future product launches. Here are some examples but the full report can be found here.\nAnalysis was conduct with following components:\n\nData Cleaning and Validation: first I conducted a validation and cleaning, including correcting categorical inconsistencies, handling missing values, range validation, and ensuring data type consistency.\nExploratory Analysis: here I focused on comprehending the performance of the sales methods over time, specifically examining their impact on sales volume across different states. Additionally, I also analyzed whether loyal customers or new customers showed more interest in the new product offerings.\nKey Business Metric Identification: the key business metric the company continue utilize is sales over time attributed to each method and the comparison of sales in loyal and occasional customers after a product launch.\nRecommendation:\n\nThe revenue experienced exponential growth during the first six weeks. Therefore, continuous monitoring of different sales methods over time is essential to assess their ongoing performance.\nIt is evident that the “Call” method generates the least revenue despite requiring the most employee time. Therefore, it is recommended to significantly reduce the use of this method and focus more on the “Email + Call” and “Email” methods.\nNewer customers tend to purchase more new products, indicating that a marketing plan targeting new customers would be highly beneficial.\nHigh-paying customers (fewer than 500 in total) should be incentivized. Additionally, marketing efforts should target these customers based on their locations to maximize effectiveness.\n\nPresentation: Then the findings were presented through a 12 minute recorded video.\n\n\n\nCode\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nsetwd(\"C:/Users/udwal/Documents/Studies_Viadrina/Becoming Data Fluent/Fluent2024/Bonus Assignment/Portfolio\")\n\n# Importing data\ndata &lt;- read_csv(\"product_sales.csv\")\n\n#  Checking uneque customer IDs\nnum_unique_customers &lt;- length(unique(data$customer_id))\n\n# Checking column unique categorical values \nunique_methods &lt;- unique(data$sales_method)\n\n# fixing typos\ntypo_dict &lt;- c(\n  \"em + call\" = \"Email + Call\",\n  \"email\" = \"Email\")\n\ndata &lt;- data %&gt;%\n  mutate(sales_method = coalesce(typo_dict[sales_method], sales_method))\n\n# Cehcking after fixing typos\nunique_methods &lt;- unique(data$sales_method)\nprint(unique_methods)\n\nunique_product_count &lt;- unique(data$nb_sold)\n\n# Revenue has only 7% missing data and hence droppping NAs\ndata_clean &lt;- data %&gt;% filter(!is.na(revenue))\n\n# values should not exceed 40 and hence filtering\ndata_clean &lt;- data_clean %&gt;% \n    filter(!years_as_customer &gt; 40)\n\n# site visits\nsummary(data_clean$nb_site_visits)\n\nunique_state &lt;- unique(data_clean$state)\nprint(unique_state)\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\npastel_colors &lt;- brewer.pal(6, \"Pastel2\")\n\nggplot(data_clean, aes(x = as.factor(week), y = revenue, fill = sales_method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = pastel_colors) +\n  labs(x = \"Week\", y = \"Revenue\", fill = \"Sales Method\") +\n  ggtitle(\"Revenue by Week and Sales Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data_clean, aes(y = revenue)) +\n  geom_boxplot() +\n  labs(y = \"Revenue\") +\n  ggtitle(\"Revenue Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter high-revenue customers (above 200.00)\nhigh_rev_cust &lt;- data_clean %&gt;%\n  filter(revenue &gt; 200.00)\n\n# Aggregate data by state\ndata_aggregated &lt;- high_rev_cust %&gt;%\n  group_by(state) %&gt;%\n  summarise(total_revenue = sum(revenue, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Sort data by total_revenue in descending order\ndata_aggregated &lt;- data_aggregated %&gt;%\n  arrange(desc(total_revenue))\n\n# Reorder state factor levels based on total_revenue\ndata_aggregated$state &lt;- factor(data_aggregated$state, levels = data_aggregated$state)\n\nggplot(data_aggregated, aes(x = \"\", y = state, fill = total_revenue)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"orange\", name = \"Total Revenue\") +\n  labs(x = \"State\", y = NULL, fill = \"Total Revenue\") +\n  ggtitle(\"Total Revenue by State for High Value Customers\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 6)\n  )"
  },
  {
    "objectID": "posts/certificate/index.html#certified-data-analyst-exam-submission",
    "href": "posts/certificate/index.html#certified-data-analyst-exam-submission",
    "title": "Certification Exam",
    "section": "",
    "text": "As part of the certified data analyst exam by DataCamp, I completed a two-hour exam testing my R and SQL skills, followed by a four-hour practical exam where I analyzed sales data from a stationary shop.\nThe goal of the analysis was to determine the best-performing marketing method following the launch of new products. Additionally, I identified high-paying and loyal customers to prioritize them in the marketing strategy for future product launches. Here are some examples but the full report can be found here.\nAnalysis was conduct with following components:\n\nData Cleaning and Validation: first I conducted a validation and cleaning, including correcting categorical inconsistencies, handling missing values, range validation, and ensuring data type consistency.\nExploratory Analysis: here I focused on comprehending the performance of the sales methods over time, specifically examining their impact on sales volume across different states. Additionally, I also analyzed whether loyal customers or new customers showed more interest in the new product offerings.\nKey Business Metric Identification: the key business metric the company continue utilize is sales over time attributed to each method and the comparison of sales in loyal and occasional customers after a product launch.\nRecommendation:\n\nThe revenue experienced exponential growth during the first six weeks. Therefore, continuous monitoring of different sales methods over time is essential to assess their ongoing performance.\nIt is evident that the “Call” method generates the least revenue despite requiring the most employee time. Therefore, it is recommended to significantly reduce the use of this method and focus more on the “Email + Call” and “Email” methods.\nNewer customers tend to purchase more new products, indicating that a marketing plan targeting new customers would be highly beneficial.\nHigh-paying customers (fewer than 500 in total) should be incentivized. Additionally, marketing efforts should target these customers based on their locations to maximize effectiveness.\n\nPresentation: Then the findings were presented through a 12 minute recorded video.\n\n\n\nCode\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nsetwd(\"C:/Users/udwal/Documents/Studies_Viadrina/Becoming Data Fluent/Fluent2024/Bonus Assignment/Portfolio\")\n\n# Importing data\ndata &lt;- read_csv(\"product_sales.csv\")\n\n#  Checking uneque customer IDs\nnum_unique_customers &lt;- length(unique(data$customer_id))\n\n# Checking column unique categorical values \nunique_methods &lt;- unique(data$sales_method)\n\n# fixing typos\ntypo_dict &lt;- c(\n  \"em + call\" = \"Email + Call\",\n  \"email\" = \"Email\")\n\ndata &lt;- data %&gt;%\n  mutate(sales_method = coalesce(typo_dict[sales_method], sales_method))\n\n# Cehcking after fixing typos\nunique_methods &lt;- unique(data$sales_method)\nprint(unique_methods)\n\nunique_product_count &lt;- unique(data$nb_sold)\n\n# Revenue has only 7% missing data and hence droppping NAs\ndata_clean &lt;- data %&gt;% filter(!is.na(revenue))\n\n# values should not exceed 40 and hence filtering\ndata_clean &lt;- data_clean %&gt;% \n    filter(!years_as_customer &gt; 40)\n\n# site visits\nsummary(data_clean$nb_site_visits)\n\nunique_state &lt;- unique(data_clean$state)\nprint(unique_state)\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\npastel_colors &lt;- brewer.pal(6, \"Pastel2\")\n\nggplot(data_clean, aes(x = as.factor(week), y = revenue, fill = sales_method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = pastel_colors) +\n  labs(x = \"Week\", y = \"Revenue\", fill = \"Sales Method\") +\n  ggtitle(\"Revenue by Week and Sales Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data_clean, aes(y = revenue)) +\n  geom_boxplot() +\n  labs(y = \"Revenue\") +\n  ggtitle(\"Revenue Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter high-revenue customers (above 200.00)\nhigh_rev_cust &lt;- data_clean %&gt;%\n  filter(revenue &gt; 200.00)\n\n# Aggregate data by state\ndata_aggregated &lt;- high_rev_cust %&gt;%\n  group_by(state) %&gt;%\n  summarise(total_revenue = sum(revenue, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Sort data by total_revenue in descending order\ndata_aggregated &lt;- data_aggregated %&gt;%\n  arrange(desc(total_revenue))\n\n# Reorder state factor levels based on total_revenue\ndata_aggregated$state &lt;- factor(data_aggregated$state, levels = data_aggregated$state)\n\nggplot(data_aggregated, aes(x = \"\", y = state, fill = total_revenue)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"orange\", name = \"Total Revenue\") +\n  labs(x = \"State\", y = NULL, fill = \"Total Revenue\") +\n  ggtitle(\"Total Revenue by State for High Value Customers\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 6)\n  )"
  },
  {
    "objectID": "posts/powerbi/index.html",
    "href": "posts/powerbi/index.html",
    "title": "Sales Data Analysis",
    "section": "",
    "text": "As part of the Data Analysis course, I conducted an analysis of sales data from a bike shop active in three states of Australia using PowerBI. Take a look at the raw data…\nTo enhance the validity of my findings and uncover deeper insights, I also explored external cycling trends by analyzing the National Annual Cycling Survey data from Australia. This analysis was performed using R, allowing me to cross-reference sales data with broader industry trends.\n\n\n\n\n\n\nThe project information at glance:\n\nBelow are the detailed steps of the analysis:\n\nBusiness Question: The analysis began with defining the business questions and objectives based on the availability of the dataset and its variables.\nData Cleaning and Modeling: The raw data presented several issues that required a degree of cleaning and preparation. To address these, I used DAX queries for data transformation and modeling.\nVisualization: Using PowerBI, I created four dashboards to summarize the analysis comprehensively. These dashboards focused on identifying target customer segments, uncovering patterns in their purchasing behavior, and identifying high-value and loyal customers. Then I looked into the sales performance, including profit margins for popular products and seasonal trends. This approach allowed for a detailed view of the data, facilitating deeper insights into both customer dynamics and sales performance.\nExternal Trend: Using R, I selected key variables from the dataset, prepared the data, and analyzed external trends in cycling.\nReport: Finally, I compiled the findings and recommendations into a comprehensive report. Full report is available on request."
  },
  {
    "objectID": "posts/powerbi/index.html#analysis-of-bike-sales-data-in-australia",
    "href": "posts/powerbi/index.html#analysis-of-bike-sales-data-in-australia",
    "title": "Sales Data Analysis",
    "section": "",
    "text": "As part of the Data Analysis course, I conducted an analysis of sales data from a bike shop active in three states of Australia using PowerBI. Take a look at the raw data…\nTo enhance the validity of my findings and uncover deeper insights, I also explored external cycling trends by analyzing the National Annual Cycling Survey data from Australia. This analysis was performed using R, allowing me to cross-reference sales data with broader industry trends.\n\n\n\n\n\n\nThe project information at glance:\n\nBelow are the detailed steps of the analysis:\n\nBusiness Question: The analysis began with defining the business questions and objectives based on the availability of the dataset and its variables.\nData Cleaning and Modeling: The raw data presented several issues that required a degree of cleaning and preparation. To address these, I used DAX queries for data transformation and modeling.\nVisualization: Using PowerBI, I created four dashboards to summarize the analysis comprehensively. These dashboards focused on identifying target customer segments, uncovering patterns in their purchasing behavior, and identifying high-value and loyal customers. Then I looked into the sales performance, including profit margins for popular products and seasonal trends. This approach allowed for a detailed view of the data, facilitating deeper insights into both customer dynamics and sales performance.\nExternal Trend: Using R, I selected key variables from the dataset, prepared the data, and analyzed external trends in cycling.\nReport: Finally, I compiled the findings and recommendations into a comprehensive report. Full report is available on request."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Model Fine-tuning\n\n\n\n\n\n\nPython\n\n\nLLM\n\n\nfine tune\n\n\nLlama\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHealth and Commute\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\ncommute\n\n\nhealth\n\n\n\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSales Data Analysis\n\n\n\n\n\n\nPowerBI\n\n\nR\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR Practice\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCertification Exam\n\n\n\n\n\n\nR\n\n\nSQL\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/health_commuting/index.html#this-project",
    "href": "posts/health_commuting/index.html#this-project",
    "title": "Health and Commute",
    "section": "This Project",
    "text": "This Project\nIn this project, I aim to study the relationship between Health and Commute using the German Socio-Economic Panel Survey (SOEP). The SOEP is a comprehensive longitudinal survey conducted annually that has been running since 1984, focusing on the socio-economic conditions of households in Germany. Each year, the survey collects data from approximately 30,000 individuals in around 15,000 households using mixed-mode design, including face-to-face interviews, telephone interviews, online surveys, and paper-based interviews. The data collection process is designed to ensure the survey’s longitudinal nature, allowing for the observation of social and economic changes over time. The access to the data is restricted to researchers for scientific purposes or for teaching purposes. Strict confidentiality measures are in place, including compliance with German data protection laws, prohibiting data sharing with third parties and ensuring secure storage.\n\n\n\nFig 1. SOEP Topics"
  },
  {
    "objectID": "posts/health_commuting/index.html#study-hyphothesis-and-questions",
    "href": "posts/health_commuting/index.html#study-hyphothesis-and-questions",
    "title": "Health and Commute",
    "section": "Study Hyphothesis and Questions",
    "text": "Study Hyphothesis and Questions\nThe literature review indicates that a longer commute distance is associated with a higher number of reported sick days. Based on this evidence, the study employs the following hypotheses:\nNull Hypothesis: There is no relationship between commute distance and the number of days off from work due to sickness.\nAlternative Hypothesis: A longer commute distance is associated with a higher number of days off from work due to sickness.\nFurthermore the study questions are driven as follows:\n\nHow do commuting characteristics, such as distance and time, impact various health outcomes, including sick days and current health status?\nHow does this impact vary across different demographic groups, such as age and gender, or according to various economic factors, such as income and employment status?"
  },
  {
    "objectID": "posts/health_commuting/index.html#methodology",
    "href": "posts/health_commuting/index.html#methodology",
    "title": "Health and Commute",
    "section": "Methodology",
    "text": "Methodology\nThis study builds on the work of Goerke and Lorenz (2017), who conducted a comprehensive analysis using SOEP data from the years 2003 to 2011. In their research, focused on commuting distance and sickness absence as key variables, while also considering personal characteristics and job-related indicators. Their analysis employed various econometric models, including Negative Binomial Regression and Fixed-Effects Least Squares Models.\nAlthough this study will not delve into as much detail as the aforementioned research, it will extend the analysis of the relationship between commuting distance and sickness absence using data from five years (2015 to 2019) through Mixed Effects Model Analysis. Additionally, the study will examine various demographic and economic indicators, including age, gender, income, and employment status.\nThe analysis consists of two parts. Descriptive analysis offer a snapshot of the data, providing a summary of its main variables. In contrast, mixed effects analysis delivers a deeper understanding by estimating the effects of predictors while accounting for random variability. This approach leads to more nuanced conclusions about the relationships within the data, which are crucial for decision-making and hypothesis testing.\n\\[\n  y_{ij} = \\beta_0 + \\beta_1 x_{ij1} + \\beta_2 x_{ij2} + \\ldots + \\beta_p x_{ijp} + u_{0j} + \\epsilon_{ij}\n  \\]\nDue to inconsistencies in the data, such as missing commute distance information for 2016 and 2018, the chosen model is well-suited for handling unequally spaced time points, time invariant and missing data. To avoid the influence of COVID-19, the year 2020 will not be included in the analysis, even though data for that year is available."
  },
  {
    "objectID": "posts/health_commuting/index.html#data-preparation",
    "href": "posts/health_commuting/index.html#data-preparation",
    "title": "Health and Commute",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nGeneralEmployment StatusHealth DataCommute Data\n\n\nI began by extracting data from a comprehensive dataset, focusing on variables pertinent to the study. Initially, 13 variables were identified using the Code Book (Bartels, Nachtigall, and Schwinn (2023)) and imported from a large dataset. The data was then filtered to include only individuals within the economically active age group (18-65) and for the years relevant to the study. The SOEP survey dataset includes negative codes to denote labels where data is unavailable, irrelevant, or not applicable for certain periods. To facilitate accurate analysis in R, these negative coded labels are converted to NA to represent missing data.\n\n\nShow code\nsetwd(\"C:/Users/udwal/Documents/Studies_Viadrina/Becoming Data Fluent/Fluent2024/Final Project\")\n\n# Getting individual data from pl\nsoep_df &lt;- read_dta(\"pl.dta\", \n                          col_select = c(syear, ple0055, ple0046, ple0008, ple0072, plb0158, plb0592, pid, plb0471_h, plb0031_h, plb0022_h))\n\n# Getting the individual data from pequiv that were not in pl\ndata_pequiv &lt;- read_dta(\"pequiv.dta\", \n         col_select = c(pid, syear, d11101, d11102ll))\n\n# Joining the two dataset\ncombined_df &lt;- soep_df %&gt;%\n  full_join(data_pequiv, by = c(\"pid\", \"syear\"))\n\n# Converting negative labels into NA as preparation of data\ncombined_df[combined_df &lt; 0] &lt;- NA\n\n# Renaming columns for ease of use\ncombined_df &lt;- combined_df %&gt;%\n  rename(\n    hospital_stays = ple0055,\n    sick_days_LY = ple0046,\n    health_state = ple0008,\n    doctor_visit_L3 = ple0072,\n    commute_km = plb0158,\n    commute_min = plb0592,\n    gender = d11102ll,\n    age = d11101,\n    income = plb0471_h,\n    job_change = plb0031_h,\n    employment_status = plb0022_h)\n\n# Filtering out the years only relevant to the research questions\ncombined_df &lt;- combined_df %&gt;%\n  filter(age &gt;= 18 & age &lt;= 65, syear &gt;= 2015 & syear &lt;= 2020)\n\n# sick days are recorded for the previous year, hence shifting the values to the previous year so the relationship with other variables can be analyzed without lag in the data.\ncombined_df &lt;- combined_df %&gt;%\n  mutate(syear = as.numeric(syear)) %&gt;%\n  mutate(sick_days_LY = as.numeric(sick_days_LY)) %&gt;%\n  group_by(pid) %&gt;%\n  arrange(pid, syear) %&gt;%\n  # Create the previous year variable\n  mutate(sick_days_PY = dplyr::lead(sick_days_LY, n = 1)) %&gt;%\n  # Fill NA values with the most recent non-NA value\n  fill(sick_days_PY, .direction = \"up\") %&gt;%\n  # Ungroup\n  ungroup() %&gt;%\n  # 2020 was pulled only for the reason to get 2019 sick days, hence it can be filtered out\n  filter(syear != 2020)\n\n\nSince the Sick Days values are recorded for the previous year, the Sick Days variable was shifted by one year. This adjustment allowed it to align the Sick Days values with other variables from the same year, such as commute distance. Consequently, the data for year 2020 is removed to ensure that the analysis accurately reflects the year 2019. This way, both sick days and commute distance data correspond to the same time period, enabling a more accurate analysis.\n\n\nShow code\n# similar number of participants across the years\ntable(combined_df$syear)\n\n# a majority of the participants' fist year is 2015 followed by 2017\ncombined_df %&gt;%  \n  group_by(pid) %&gt;% \n  summarise(first_year = min(syear)) %&gt;% \n  ggplot(aes(x=first_year)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"First Year of Interview per Persons\")\n\n\nShow code\n# majority of the participants' last interview was in 2019\ncombined_df %&gt;%  \n  group_by(pid) %&gt;% \n  summarise(last_year = max(syear)) %&gt;% \n  ggplot(aes(x=last_year)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"Last Year of Interview per Persons\")\n\n\n\n\nShow code\n#filtering unique IDs\nunique_ids_df &lt;- combined_df %&gt;%\n  distinct(pid, .keep_all = TRUE)\n\ndistinct_count &lt;- length(unique(combined_df$pid))\n\nmean_age &lt;- round(mean(combined_df$age), 1)\n\n# calculating gender ratio\nunique_ids_df$gender &lt;- as.numeric(unique_ids_df$gender)\n\ngender_counts &lt;- unique_ids_df %&gt;%\n  filter(!is.na(gender)) %&gt;%\n  count(gender)\n\nnum_females &lt;- gender_counts$n[gender_counts$gender == 2]\nnum_males &lt;- gender_counts$n[gender_counts$gender == 1]\n\n# Calculate gender ratio\ngender_ratio &lt;- round((num_females / num_males), 2)\n\n# Age distribution\nggplot(unique_ids_df, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"#f89089\", color = \"white\", alpha = 0.7) +\n  geom_vline(aes(xintercept = mean(age)), color = \"#32b6b8\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~syear) +\n  labs(title = \"Graph 1. Histogram of Age by Year\",\n       x = \"Age\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12),\n    plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThe initial dataset comprised 72213 observations across 13 variables, with 24314 unique personal IDs. Visualization of participants’ first and last year of interviews indicates that a high number of participants were in 2015 compared to the other years. However this was balanced in the final dataset after the cleaning process which resulted majority participated throughout all five years of the study period. This continuity is crucial for ensuring unbiased results. The participants average age is 40.8 and the gender ratio is 0.99, indicating a slightly lower number of female participants compared to male participants.\n\n\n\n\nShow code\n# look at the percentage of unemployed and NAs each year\n# Calculate the percentage of \"Nicht erwerbstaetig\" by year\ncombined_percentage_by_year &lt;- unique_ids_df %&gt;%\n  group_by(syear) %&gt;%\n  summarize(\n    total = n(),\n    combined_count = sum(as_factor(employment_status) %in% c(\"[9] Nicht erwerbstaetig\") | \n                         is.na(employment_status), \n                         na.rm = TRUE),\n    combined_percentage = round((combined_count / total) * 100, 2))\n\nmean_percent &lt;- round(mean(combined_percentage_by_year$Percent), 2)\n\n# Rename columns for display in the table\ncolnames(combined_percentage_by_year) &lt;- c(\"Year\", \"Total Participants\", \"Unemployed Count\", \"Percent\")\n\nkable(combined_percentage_by_year, \n      caption = \"Table 1. Percentage of unemployed and NAs\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n  row_spec(0, background = \"#f9918a\", color = \"white\", bold = TRUE)\n\n\n\n\nTable 1. Percentage of unemployed and NAs\n\n\nYear\nTotal Participants\nUnemployed Count\nPercent\n\n\n\n\n2015\n12356\n4028\n32.60\n\n\n2016\n3801\n3135\n82.48\n\n\n2017\n4220\n2602\n61.66\n\n\n2018\n1394\n870\n62.41\n\n\n2019\n2543\n1220\n47.97\n\n\n\n\n\n\n\n\n\n\nShow code\n# check unique job change labels\nprint(unique(as_factor(unique_ids_df$job_change)))\n\n# count job changes in the year: about 11% changed job\njob_change_count_by_year &lt;- unique_ids_df %&gt;%\n  group_by(syear) %&gt;%\n  summarize(\n    total = n(),\n    ja_count = sum(as_factor(job_change) == \"[1] Ja\", na.rm = TRUE),\n    ja_percentage = (ja_count / total) * 100) %&gt;%\n  arrange(syear)\n\n# Print job change table\nprint(job_change_count_by_year)\n\n\n\n\nShow code\n# 3: Work-from-Home Status We have data on work-from-home (WFH) status only for the year 2020, with no records of WFH in previous years. Since 2020 is not the primary focus of this study, we assume that WFH was not an option during the years of interest. Consequently, we will exclude the WFH information from the dataset.\n\n# 4: Removing outlieres in Income\nggplot(combined_df, aes(x = income)) +\n  geom_boxplot()\n\n\nShow code\ncombined_df &lt;- combined_df %&gt;%\n  filter(income &lt;= 15000)\n\n\nOn average, NA of participants each year are either unemployed or have missing data for other reasons. Additionally, an average of 8.93 of participants change jobs (employers) each year. Given the high rate of job changes, it is important to also analyze changes in workplace location. Although the SOEP does not record data on workplace changes directly, this can be inferred from the commute distance to work. The survey question asks: \"How far (in kilometers) is it from where you live to where you work?\" Any change in workplace is expected to be reflected in changes to this distance. Notably, there is no recorded data on working from home (WFH) for the years of interest in this analysis.\n\n\nAccording to the German Statistics Office, the average number of sickness absence days in 2019 was 11. To ensure the validity of our analysis, we have addressed outliers in the data. We assume that extreme values of sick days could either indicate chronic illness or data entry errors. Therefore, we have excluded sickness absence days exceeding two months from our dataset.\n\n\nShow code\n# after filtering out unemployed participants and negative labels\ncombined_df$employment_status &lt;- as.factor(combined_df$employment_status)\n\nfiltered_data &lt;- combined_df %&gt;%\n  filter(!is.na(employment_status) & employment_status != \"[9] Nicht erwerbstaetig\")\n\nhealth_variables &lt;- filtered_data %&gt;%\n  filter(!is.na(sick_days_PY) & !is.na(hospital_stays) & !is.na(doctor_visit_L3) & !is.na(health_state)) %&gt;%\n  mutate(across(c(hospital_stays, sick_days_LY, doctor_visit_L3, health_state), ~as.numeric(zap_labels(.))))\n\n# plotting to see any outliers\nlong_df &lt;- health_variables %&gt;%\n  pivot_longer(cols = c(hospital_stays, doctor_visit_L3, sick_days_PY, health_state),\n               names_to = \"variable\",\n               values_to = \"value\")\n\nggplot(long_df, aes(x = variable, y = value)) +\n  geom_boxplot(fill = \"#32b6b8\", color = \"#f78f88\") +\n  facet_wrap(~variable, scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  labs(title = \"Graph 2. Distribution of Health-related Variables\",\n       x = \"\",\n       y = \"Value\") +\n  theme(axis.text.x = element_blank(),\n        strip.text = element_text(size = 12, face = \"bold\"),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nTo select the key health variables for analysis, we examined the trends over time for four variables: 1. Doctor visits in the last 3 months, 2. Number of hospital stays, 3. Number of sickness absence days, and 4. Self-reported health status, ranging from 1 (highest) to 5 (lowest). The trends for these variables did not show significant similarities, and none of the variables appeared anomalous. Consequently, we selected the “Number of sickness absence days” and “Self-reported health status” for the analysis, as these are commonly used by researchers, including Goerke and Lorenz (2017), in their studies utilizing SOEP data.\n\n\nShow code\n# Calculate summary statistics for each variable\ndv_summary &lt;- summary(health_variables$doctor_visit_L3)\nhos_summary &lt;- summary(health_variables$hospital_stays)\nsd_summary &lt;- summary(health_variables$sick_days_PY)\nhes_summary &lt;- summary(health_variables$health_state)\n\n# Combine the results into a data frame\nsummary_table &lt;- data.frame(\n  Statistic = names(dv_summary),\n  Doctor_Visits_L3 = as.numeric(dv_summary),\n  Hospital_Stays = as.numeric(hos_summary),\n  Sick_Days_PY = as.numeric(sd_summary),\n  Health_Status = as.numeric(hes_summary))\n\n# Create and display a table using gt\nsummary_table_gt &lt;- summary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Table 2. Summary Statistics for Health Variables\") %&gt;%\n  cols_label(\n    Statistic = \"Statistic\",\n    Doctor_Visits_L3 = \"Doctor Visits (L3)\",\n    Hospital_Stays = \"Hospital Stays\",\n    Sick_Days_PY = \"Sick Days\",\n    Health_Status = \"Health Status\") %&gt;%\n  fmt_number(\n    columns = vars(Doctor_Visits_L3, Hospital_Stays, Sick_Days_PY, Health_Status),\n    decimals = 1) %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f78f88\"),\n      cell_text(weight = \"bold\", color = \"white\")),\n    locations = cells_column_labels(\n      columns = everything()))\n\n# Print the gt table\nsummary_table_gt\n\n\n\n\n\n\n\n\nTable 2. Summary Statistics for Health Variables\n\n\nStatistic\nDoctor Visits (L3)\nHospital Stays\nSick Days\nHealth Status\n\n\n\n\nMin.\n1.0\n1.0\n1.0\n1.0\n\n\n1st Qu.\n2.0\n1.0\n5.0\n2.0\n\n\nMedian\n3.0\n1.0\n13.0\n3.0\n\n\nMean\n4.8\n1.4\n36.8\n3.0\n\n\n3rd Qu.\n5.0\n1.0\n30.0\n4.0\n\n\nMax.\n80.0\n100.0\n365.0\n5.0\n\n\n\n\n\n\n\n\n\nShow code\n# Trend over time by health variables\nggplot(long_df, aes(x = syear, y = value)) +\n  geom_line(size = 1, color = \"#32b6b8\") +\n  facet_wrap(~ variable, scales = \"free_y\", ncol = 2) +\n  labs(title = \"Graph 3. Trend of Average Health Variables Over Time\",\n       x = \"Year\",\n       y = \"Average Value\") +\n  theme_minimal() +\n  theme(strip.background = element_rect(fill = \"#f9918a\"),\n        strip.text = element_text(face = \"bold\", color = \"white\"))\n\n\n\n\n\n\n\n\n\nShow code\n#removing outliers left 16514 observations\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(sick_days_PY) & !is.na(health_state), sick_days_PY &lt; 60) %&gt;%\n  mutate(across(c(sick_days_PY, health_state), ~as.numeric(zap_labels(.))))\n\nfiltered_data &lt;- filtered_data %&gt;%\n  select(-hospital_stays, -doctor_visit_L3, -sick_days_LY)\n\n\nAfter filtering the data for health variables and excluding the two health variables no longer needed for the study, total of 21899 observations across 11 variables remained.\n\n\n\n\nShow code\n# categorizing commute distance.\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(commute_km &lt;= 200) %&gt;% # removing one outlier above 200 km\n  mutate(commute_category = case_when(\n    commute_km &gt;= 0 & commute_km &lt; 10  ~ \"Non-commuter\",\n    commute_km &gt;= 10 & commute_km &lt; 25 ~ \"Short Distance\",\n    commute_km &gt;= 25 & commute_km &lt; 50 ~ \"Medium Distance\",\n    commute_km &gt;= 50                   ~ \"Long Distance\",\n    TRUE                               ~ NA_character_))\n\n# average travel time by category of distance\nmean_commute_by_category &lt;- filtered_data %&gt;%\n  group_by(commute_category) %&gt;%\n  summarize(mean_commute_min = mean(commute_min, na.rm = TRUE)) %&gt;%\n  mutate(mean_commute_min = round(mean_commute_min, 1)) %&gt;%  \n  rename(\n    \"Commuter Type\" = commute_category,\n    \"Avg Commute Time (min)\" = mean_commute_min) %&gt;%\n  arrange(desc(`Avg Commute Time (min)`))\n\n# just extracting the value\nlong_dist_time &lt;- mean_commute_by_category %&gt;%\n  filter(`Commuter Type` == \"Long Distance\") %&gt;%\n  summarise(mean_travel_time = mean(`Avg Commute Time (min)`, na.rm = TRUE)) %&gt;%\n  pull(mean_travel_time)\n\n# just extracting the value  \nnon_commuter_time &lt;- mean_commute_by_category %&gt;%\n  filter(`Commuter Type` == \"Non-commuter\") %&gt;%\n  summarise(mean_travel_time = mean(`Avg Commute Time (min)`, na.rm = TRUE)) %&gt;%\n  pull(mean_travel_time)\n  \nkable(mean_commute_by_category, \n      caption = \"Table 3. Avg Commute Time by Commuter Type\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n  row_spec(0, background = \"#f9918a\", color = \"white\", bold = TRUE)\n\n\n\n\nTable 3. Avg Commute Time by Commuter Type\n\n\nCommuter Type\nAvg Commute Time (min)\n\n\n\n\nLong Distance\n71.4\n\n\nMedium Distance\n43.1\n\n\nShort Distance\n27.8\n\n\nNon-commuter\n14.0\n\n\n\n\n\n\n\n\nFirst the commute distance is categorized based on the same categorization as Goerke and Lorenz (2017). Then average travel time by different commute distance category was calculated. Which shows Long Distance Commuter traveling on average 71.4 minutes one way whereas people commute up to 10 kms with on average 14 minutes of travel time.\n\n\nShow code\n# removing outlier in commute time before plotting relationship with commute distance\n\nmean_min &lt;- filtered_data %&gt;%\n  summarize(mean = mean(commute_min, na.rm = TRUE)) %&gt;%\n  pull(mean)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(commute_min &lt; 150)\n\nggplot(filtered_data, aes(x = commute_km, y = commute_min)) +\n  geom_point(color = \"#f9918a\") +\n  theme_minimal() +\n  labs(title = \"Graph 4. Relationship Between Commute Distance and Time\", x = \"Commute distance in km\", y = \"Commute time in minutes\") + \n  geom_smooth(method = \"loess\", span = 0.8, se = FALSE, color = \"#32b6b8\")\n\n\n\n\n\n\n\n\n\nShow code\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(pid = as_factor(pid)) %&gt;%\n  mutate(pid = as.character(pid))\n\n# each additional km traveled increases the commute time by approximately 1.15 minutes.  \nmodel_ME &lt;- lmer(commute_min ~ commute_km - 1 + (1 | pid), data = filtered_data)\n\nmodel_summary &lt;- summary(model_ME)\n\ncoefficients_df &lt;- as.data.frame(model_summary$coefficients)\ncolnames(coefficients_df) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\")\n\n# Display the coefficients with kable and apply styling\nkable(coefficients_df, caption = \"Table 4. Model Summary: Coefficients\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n  row_spec(0, background = \"#f9918a\", color = \"white\", bold = TRUE)\n\n\n\n\nTable 4. Model Summary: Coefficients\n\n\n\nEstimate\nStd. Error\nt value\n\n\n\n\ncommute_km\n1.147848\n0.0077631\n147.8588\n\n\n\n\n\n\n\n\nLastly, relationship between commute distance and travel time was analyzed. The linear mixed model analyzing the relationship between commute distance (in kilometers) and commute time (in minutes), while accounting for individual differences. The model shows a strong positive relationship between distance and time, with each kilometer of commute distance associated with an increase of about 1.2 minutes in commute time (fixed effect estimate). The random effects indicate substantial variation between individuals (pid), with a standard deviation of 13.73 minutes which is (51.81) of the mean, suggesting a significant degree of variability relative to the average commute time. These could be factors specific to each person (like mode of transportation or route choices) play a significant role in determining commute time (Liao et al. (2020)). The high t-value (164.8) for the fixed effect suggests that the relationship between distance and time is statistically significant. Overall, this model demonstrates that while commute distance is a strong predictor of commute time, individual factors also contribute substantially to the variation in commute duration."
  },
  {
    "objectID": "posts/health_commuting/index.html#final-dataset",
    "href": "posts/health_commuting/index.html#final-dataset",
    "title": "Health and Commute",
    "section": "Final Dataset",
    "text": "Final Dataset\nAfter the data preparation stage, the dataset consists of 12 columns, including 11 original variables and one additional categorical column. The dataset contains 11698 observations that have been filtered and cleaned to ensure a balanced dataframe with no missing values. The variables are categorized into three groups: Health, Commute, and Socio-economic.\nThe analysis uses Sickness absence notified to the employer as the dependent variable, with Self-reported health status serving as a reference variable. After identifying a strong positive linear relationship between commute distance and commute time, commute distance is selected as the independent variable. Additionally, the analysis considers other variables such as gender, age, income, employment status, and employer change."
  },
  {
    "objectID": "posts/health_commuting/index.html#descriptive-analysis",
    "href": "posts/health_commuting/index.html#descriptive-analysis",
    "title": "Health and Commute",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis\n\n\nShow code\n# creating custom function\ncustom_summary &lt;- function(x) {\n  # Calculate summary statistics\n  result &lt;- c(\n    Mean = round(mean(x, na.rm = TRUE), 2),\n    SD = round(sd(x, na.rm = TRUE), 2),\n    Min = round(min(x, na.rm = TRUE), 2),\n    Max = round(max(x, na.rm = TRUE), 2))\n  return(result)}\n\n# Compute summary statistics for each category\nfull_sample &lt;- custom_summary(filtered_data$sick_days_PY)\nfull_sample_count &lt;- nrow(filtered_data)  \nfull_sample_percentage &lt;- 100  \n\nnon_commuter &lt;- filtered_data %&gt;%\n  filter(commute_category == \"Non-commuter\") %&gt;%\n  pull(sick_days_PY) %&gt;%\n  custom_summary()\n\nshort_distance &lt;- filtered_data %&gt;%\n  filter(commute_category == \"Short Distance\") %&gt;%\n  pull(sick_days_PY) %&gt;%\n  custom_summary()\n\nmedium_distance &lt;- filtered_data %&gt;%\n  filter(commute_category == \"Medium Distance\") %&gt;%\n  pull(sick_days_PY) %&gt;%\n  custom_summary()\n\nlong_distance &lt;- filtered_data %&gt;%\n  filter(commute_category == \"Long Distance\") %&gt;%\n  pull(sick_days_PY) %&gt;%\n  custom_summary()\n\n# Number and percentage of participants by commuter category\nnumber_commuter &lt;- filtered_data %&gt;%\n  group_by(commute_category) %&gt;%\n  summarize(n = n(), .groups = 'drop') %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 2))\n\n# Create a data frame for summary statistics\nsummary_stats &lt;- data.frame(\n  Category = c(\"Full Sample\", \"Non-commuter\", \"Short Distance\", \"Medium Distance\", \"Long Distance\"),\n  Mean = c(full_sample[\"Mean\"], non_commuter[\"Mean\"], short_distance[\"Mean\"], medium_distance[\"Mean\"], long_distance[\"Mean\"]),\n  SD = c(full_sample[\"SD\"], non_commuter[\"SD\"], short_distance[\"SD\"], medium_distance[\"SD\"], long_distance[\"SD\"]),\n  Min = c(full_sample[\"Min\"], non_commuter[\"Min\"], short_distance[\"Min\"], medium_distance[\"Min\"], long_distance[\"Min\"]),\n  Max = c(full_sample[\"Max\"], non_commuter[\"Max\"], short_distance[\"Max\"], medium_distance[\"Max\"], long_distance[\"Max\"]),\n  Count = c(full_sample_count, \n            number_commuter %&gt;% filter(commute_category == \"Non-commuter\") %&gt;% pull(n),\n            number_commuter %&gt;% filter(commute_category == \"Short Distance\") %&gt;% pull(n),\n            number_commuter %&gt;% filter(commute_category == \"Medium Distance\") %&gt;% pull(n),\n            number_commuter %&gt;% filter(commute_category == \"Long Distance\") %&gt;% pull(n)),  \n  Percentage = c(full_sample_percentage, \n                 number_commuter %&gt;% filter(commute_category == \"Non-commuter\") %&gt;% pull(percentage),\n                 number_commuter %&gt;% filter(commute_category == \"Short Distance\") %&gt;% pull(percentage),\n                 number_commuter %&gt;% filter(commute_category == \"Medium Distance\") %&gt;% pull(percentage),\n                 number_commuter %&gt;% filter(commute_category == \"Long Distance\") %&gt;% pull(percentage)))\n\n# Create the table using gt\ntable_gt &lt;- summary_stats %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Table 5. Summary Statistics of Sick Days by Commuter Category\") %&gt;%\n  fmt_number(columns = vars(Mean, SD, Min, Max, Count, Percentage), decimals = 2) %&gt;%\n  fmt_number(columns = vars(Count), decimals = 0) %&gt;%\n  cols_label(Category = \"Commute Category\",\n             Mean = \"Mean\",\n             SD = \"SD\",\n             Min = \"Min\",\n             Max = \"Max\",\n             Count = \"Number of Participants\",\n             Percentage = \"Percentage (%)\") %&gt;%\n  tab_spanner(\n    label = \"Summary Statistics\",\n    columns = vars(Mean, SD, Min, Max)) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = vars(Mean, SD, Min, Max, Count, Percentage)) %&gt;%\n  fmt_markdown(\n    columns = vars(Category),\n    rows = which(!is.na(summary_stats$Count))) %&gt;%\n   tab_style(\n    style = list(\n      cell_fill(color = \"#f78f88\"),\n      cell_text(weight = \"bold\", color = \"white\")),\n    locations = cells_column_labels(columns = everything())) %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f78f88\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_column_spanners(spanners = \"Summary Statistics\"))\n\ntable_gt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5. Summary Statistics of Sick Days by Commuter Category\n\n\nCommute Category\nSummary Statistics\nNumber of Participants\nPercentage (%)\n\n\nMean\nSD\nMin\nMax\n\n\n\n\nFull Sample\n10.50\n10.23\n1.00\n59.00\n11,698\n100.00\n\n\nNon-commuter\n10.66\n10.44\n1.00\n59.00\n5,272\n45.07\n\n\nShort Distance\n10.45\n10.05\n1.00\n56.00\n3,836\n32.79\n\n\nMedium Distance\n10.20\n9.98\n1.00\n58.00\n2,014\n17.22\n\n\nLong Distance\n10.42\n10.20\n1.00\n58.00\n576\n4.92\n\n\n\n\n\n\n\nOnly 4.9% of the total participants are long-distance commuters, traveling more than 50 kilometers one way to work. Table 4 compares the average number of sick days across different commuter categories and does not show significant differences between them. However, high standard deviations across all categories indicate a wide range of commute distances within each group.\n\n\nShow code\n# gender difference in commute distance and sick days\ngender_labels &lt;- c(\n  \"1\" = \"Male\",\n  \"2\" = \"Female\")\n\n# Apply the mapping\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(gender_label = factor(gender, levels = names(gender_labels), labels = gender_labels), na.rm = TRUE) %&gt;%\n  filter(!is.na(gender_label))\n\n# commute distance histogram\nggplot(filtered_data, aes(x = commute_km, fill = gender_label)) +\n  geom_density(alpha = 0.8, color = \"white\") +\n  theme_minimal() +\n  labs(\n    title = \"Graph 5. Commute Distance Distribution by Gender\",\n    x = \"Commute Distance (km)\",\n    y = \"Density\",\n    fill = \"Gender\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nGraph 5 shows that women commute significantly more than men within 20 kilometers, while men tend to commute longer distances. These findings align with the results from Chidambaram and Scheiner (2021), which explain that men are more likely to drive for longer commutes. In contrast, women often engage in trip chaining (combining multiple trips) due to childcare responsibilities, which can affect their overall commuting behavior.\n\n\nShow code\n# sick days histogram\nggplot(filtered_data, aes(sick_days_PY, fill = gender_label)) +\n  geom_density(alpha = 0.7, color = \"white\") +\n  theme_minimal() +\n  labs(\n    title = \"Graph 6. Sickness Absence in Days\", \n    x = \"Sick Days\",\n    y = \"Density\",\n    fill = \"Gender\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nIn contrast, men tend to take shorter sick leave (fewer than 10 days), while women are more likely to take longer sick leave. This is interesting because many studies show that women generally have higher rates of sickness absence compared to men. For instance, a study by Gimenez-Nadal, Molina, and Velilla (2022) found that using data from2011 to 2017, a 1% increase in workers’ daily commute correlates with an increase in annual sickness absence of 0.018% for male workers and 0.027% for female workers.\n\n\nShow code\nggplot(filtered_data, aes(x = factor(health_state), y = commute_km)) +\n  geom_boxplot(color = \"#f9918a\") +\n  theme_minimal() +\n  labs(title = \"Graph 7. Commute Distance by Health Status\",\n       x = \"Health Status\", \n       y = \"Commute Distance (km)\") +\n  coord_cartesian(ylim = c(0, 100))\n\n\n\n\n\n\n\n\n\nGraph 7 demonstrates significant variability in commute distances, particularly among individuals categorized under “very good” and “satisfactory” health states. This suggests that people within these health categories may have diverse commuting behaviors or requirements. Interestingly, individuals who self-report as being in a “bad” health state tend to commute shorter distances. This observation could suggest a case of reverse causation, where long commutes do not necessarily deteriorate health; rather, those with poorer health may choose, or be forced, to undertake shorter commutes (Raza et al. (2021)).\n\n\nShow code\n# commuter type by employment status\n# giving English labels as the original is in German\nemployment_status_labels &lt;- c(\n  \"1\" = \"Full-time\",\n  \"2\" = \"Part-time\",\n  \"3\" = \"Ausbildung\",\n  \"4\" = \"Low-income earner\",\n  \"8\" = \"Workshop for handicapped\")\n\n# Apply the mapping\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(employment_status = factor(employment_status, levels = names(employment_status_labels), labels = employment_status_labels))\n\nsummarized_data &lt;- filtered_data %&gt;%\n  group_by(commute_category, employment_status) %&gt;%\n  summarize(count = n(), .groups = 'drop')\n\nggplot(summarized_data, aes(x = employment_status, y = count, fill = commute_category)) +\n  geom_col(position = \"stack\") +  \n  theme_minimal() +\n  labs(\n    title = \"Graph 8. Employment Status by Commute Categories\",\n    x = \"Employment Status\",  \n    y = \"Count\",              \n    fill = \"Commute Category\") +\n  scale_fill_manual(values = c(\"#80bcd2\", \"#f1dedb\", \"#f9918a\", \"#32b6b8\", \"#a7641c\")) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nFull-time employees constitute the largest group of long- and medium-distance commuters. In contrast, low-income earners and individuals undergoing Ausbildung typically commute less than 25 km. According to Goerke and Lorenz (2017), this may be because full-time employees are more likely to be compensated for long commutes, making the trade-off between private time and commuting more acceptable.\n\n\nShow code\n# income and commuter type correlation: weak postive at cor = 0.19\ncor_km_income &lt;- cor(filtered_data$commute_km, filtered_data$income, use = \"complete.obs\")\n\nggplot(filtered_data, aes(x = commute_km, y = income)) +\n  geom_point(color = \"#f9918a\") +\n  geom_smooth(method = \"loess\", span = 0.8, se = FALSE, color = \"#32b6b8\") +\n  theme_minimal() +\n  labs(title = \"Graph 9. Commute Distance and Income Correlation\", x= \"Commute Ditsance (km)\", y = \"Income\")\n\n\n\n\n\n\n\n\n\nShow code\nAvg_km &lt;- filtered_data %&gt;%\n  group_by(syear) %&gt;%\n  summarize(mean_km = round(mean(commute_km, na.rm = TRUE), 2))\n\nfiltered_data &lt;- filtered_data %&gt;%\n  select(-na.rm)\n\nkable(Avg_km, \n      col.names = c(\"Year\", \"km\"), \n      caption = \"Table 6. Average Commute Distance\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n  row_spec(0, background = \"#f9918a\", color = \"white\", bold = TRUE)\n\n\n\n\nTable 6. Average Commute Distance\n\n\nYear\nkm\n\n\n\n\n2015\n15.35\n\n\n2017\n15.96\n\n\n2019\n16.00\n\n\n\n\n\n\n\n\nIn terms of income, it shows a correlation coefficient of 0.19, indicating weak positive correlation. However, commuting distances up to an average of 100 km are generally associated with higher correlation. Beyond this distance, income tends to decline as the distance increases.\nIn our data, the average commuting trip length did not see a significant increase between 2015 and 2019. The average one-way commuting distance of workers is 15.75 kilometres. This is in line with a range of other studies employing German data (Hedemann (n.d.)). Hence, our sample selection is likely unrelated to commuting behaviour."
  },
  {
    "objectID": "posts/health_commuting/index.html#mixed-effects-analysis",
    "href": "posts/health_commuting/index.html#mixed-effects-analysis",
    "title": "Health and Commute",
    "section": "Mixed Effects Analysis",
    "text": "Mixed Effects Analysis\nIn this analysis, I used a statistical technique a linear mixed model to explore how different factors affect the number of sick days people take from work. This model helps to consider both individual differences and overall trends. I included variables such as commute distance, income, age, job changes, and whether individuals work full-time. I also accounted for changes over time by incorporating the year into the model. To ensure our results were accurate, I standardized these factors, which means I compared them on a common scale. This approach allowed me to understand how each factor, like a longer commute or higher income, influences sick days while also accounting for variations among different people. The main health variable is sickness absence days (Model1), but also to compare I included Self-reported Health State (Model2) in the analysis.\n\n\nShow code\n#Creating dummy variable for job change\nfiltered_data$job_change &lt;- as.numeric(filtered_data$job_change)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(job_change_dummy = ifelse(job_change == 1, 1, 0),\n         gender_dummy = ifelse(gender_label == \"Male\", 1, 0),\n         full_time_dummy = ifelse(employment_status == \"Full-time\", 1, 0))\n\n# linear mixed-effects models fitting \n# Prepare the data by scaling the continues variables\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(\n    commute_km_z = scale(commute_km),\n    age_z = scale(age),\n    income_z = scale(income),\n    syear_z = scale(syear))\n\n# Model 1: Number of Days Off Work\nmodel1 &lt;- lmer(sick_days_PY ~ commute_km_z + gender_dummy + age_z + income_z + \n               job_change_dummy + full_time_dummy + syear_z + (1|pid), \n               data = filtered_data)\n\n\n# Model 2: Current Health State\nmodel2 &lt;- lmer(health_state ~ commute_km_z + gender_dummy + age_z + income_z + \n               job_change_dummy + full_time_dummy + syear_z + (1|pid), \n               data = filtered_data)\n\n# Check summaries\nsummary(model1)\nsummary(model2)\n\n\nBoth models analyze data from 11,495 observations across 6,649 unique individuals. The variables were standardized to ensure they are on a common scale for analysis. To interpret the results in the context of the original data, the effects are expressed in terms of standard deviations of the variables. The key findings are as follows:\n\nThe much larger random effects in Model 1 suggest that sickness absence varies more between and within individuals compared to self-reported health state.\nLonger commutes are associated with slightly more sick days, every 17 km increase in distance associated with 2.7 more sick days but have minimal impact on self-reported health. However the findings are statistically not significant.\nGender has a small negative effect on sick days (males taking fewer sick days (0.3) than women) but negligible effect on self-reported health.\nAge is positively associated with both more sick days (every 11 years increase in age associated with people take about 7.4 more sick days) and worse self-reported health, with a stronger relative effect on health status.\nHigher income is associated with fewer sick days and better self-reported health. The effect is relatively strong in both models.\nJob change is associated with fewer sick days but has minimal impact on self-reported health.\nFull-time employment is strongly associated with more sick days (1.49 more sick days than other groups) but has negligible impact on self-reported health.\nThere’s a trend of increasing sick days (0.8 days every 1.6 years) over time, but minimal change in self-reported health status.\nThe models show different sensitivity to predictors, with sickness absence (Model 1) being more responsive to work-related factors (full-time status, job change) compared to self-reported health status (Model 2).\n\n\n\nShow code\n# tidying model result\nfixed_effects1 &lt;- tidy(model1, effects = \"fixed\")\nfixed_effects2 &lt;- tidy(model2, effects = \"fixed\")\n\n# Creating result table\nsimple_table &lt;- bind_rows(\n  fixed_effects1 %&gt;% mutate(Model = \"Model 1\"),\n  fixed_effects2 %&gt;% mutate(Model = \"Model 2\")) %&gt;%\n  mutate(\n    Variable = case_when(\n      term == \"commute_km_z\" ~ \"Commute Distance\",\n      term == \"gender_dummy\" ~ \"Gender\",\n      term == \"age_z\" ~ \"Age\",\n      term == \"income_z\" ~ \"Income\",\n      term == \"job_change_dummy\" ~ \"Job Change\",\n      term == \"full_time_dummy\" ~ \"Full-time Status\",\n      term == \"syear_z\" ~ \"Year\",\n      term == \"(Intercept)\" ~ \"Intercept\"),\n    Effect = ifelse(statistic &gt; 1.96 | statistic &lt; -1.96, \n                    ifelse(estimate &gt; 0, \"Increase\", \"Decrease\"), \n                    \"No significant effect\"),\n    Significance = ifelse(abs(statistic) &gt; 3.291, \"***\",\n                   ifelse(abs(statistic) &gt; 2.576, \"**\",\n                   ifelse(abs(statistic) &gt; 1.96, \"*\", \"\"))),\n    `Relative Impact` = abs(estimate) / max(abs(estimate))) %&gt;%\n  select(Model, Variable, Effect, Significance, `Relative Impact`) %&gt;%\n  arrange(Model, desc(`Relative Impact`))\n\n# Display the table\nkable(simple_table, format = \"html\", escape = FALSE, caption = \"Table 7. Mixed Effect Model Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), full_width = TRUE) %&gt;%\n  column_spec(5, width = \"100px\",\n              color = \"white\",\n              background = spec_color(simple_table$`Relative Impact`, end = 0.7)) %&gt;%\n  row_spec(0, background = \"#f9918a\", color = \"white\", bold = TRUE) %&gt;%  \n  row_spec(9, extra_css = \"border-top: 2px solid black;\")\n\n\n\nTable 7. Mixed Effect Model Summary\n\n\nModel\nVariable\nEffect\nSignificance\nRelative Impact\n\n\n\n\nModel 1\nIntercept\nIncrease\n***\n1.0000000\n\n\nModel 1\nFull-time Status\nIncrease\n***\n0.1501354\n\n\nModel 1\nIncome\nDecrease\n***\n0.1455591\n\n\nModel 1\nJob Change\nDecrease\n**\n0.0748561\n\n\nModel 1\nAge\nIncrease\n***\n0.0684733\n\n\nModel 1\nYear\nIncrease\n***\n0.0513631\n\n\nModel 1\nGender\nNo significant effect\n\n0.0316477\n\n\nModel 1\nCommute Distance\nNo significant effect\n\n0.0159263\n\n\nModel 2\nIntercept\nIncrease\n***\n0.2438212\n\n\nModel 2\nAge\nIncrease\n***\n0.0205941\n\n\nModel 2\nIncome\nDecrease\n***\n0.0112824\n\n\nModel 2\nJob Change\nNo significant effect\n\n0.0017290\n\n\nModel 2\nCommute Distance\nNo significant effect\n\n0.0008270\n\n\nModel 2\nYear\nNo significant effect\n\n0.0007459\n\n\nModel 2\nFull-time Status\nNo significant effect\n\n0.0004177\n\n\nModel 2\nGender\nNo significant effect\n\n0.0002917"
  },
  {
    "objectID": "posts/health_commuting/index.html#variables-of-the-study",
    "href": "posts/health_commuting/index.html#variables-of-the-study",
    "title": "Health and Commute",
    "section": "Variables of the study",
    "text": "Variables of the study\n\nCommute Distance in kilometers\n\nSickness Absence Day\n\nSelf Reported Health\n\nEmployment Status\n\nIncome of Individual\n\nCommute Time (represented by commute distance)\nAge\nGender\nYear\nJob Change (employer)\nPID\n\nThe full details of variables can be found in Topics of SOEP-Core."
  },
  {
    "objectID": "posts/health_commuting/index.html#graphics-and-results-that-were-in-the-background",
    "href": "posts/health_commuting/index.html#graphics-and-results-that-were-in-the-background",
    "title": "Health and Commute",
    "section": "Graphics and Results that were in the background",
    "text": "Graphics and Results that were in the background\nFirst and last interview year of the participants\n\n\nShow code\n# a maority of the participants' fist year is 2015 followed by 2017\ncombined_df %&gt;%  \n  group_by(pid) %&gt;% \n  summarise(first_year = min(syear)) %&gt;% \n  ggplot(aes(x=first_year)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"First Year of Interview per Persons\")\n\n\n\n\n\n\n\n\n\nShow code\n# majority of the participants' last interview was in 2019\ncombined_df %&gt;%  \n  group_by(pid) %&gt;% \n  summarise(last_year = max(syear)) %&gt;% \n  ggplot(aes(x=last_year)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"Last Year of Interview per Persons\")\n\n\n\n\n\n\n\n\n\nNumber of participants each year\n\n\nShow code\n# similar number of participants across the years\ntable(combined_df$syear)\n\n\n\n2015 2016 2017 2018 2019 \n7001 6360 7315 7190 6929 \n\n\nJob change percentage by year\n\n\nShow code\nkable(job_change_count_by_year)\n\n\n\n\n\nsyear\ntotal\nja_count\nja_percentage\n\n\n\n\n2015\n12356\n1479\n11.969893\n\n\n2016\n3801\n160\n4.209419\n\n\n2017\n4220\n305\n7.227488\n\n\n2018\n1394\n201\n14.418938\n\n\n2019\n2543\n173\n6.802989\n\n\n\n\n\nIncome outliers\n\n\nShow code\nggplot(combined_df, aes(x = income)) +\n  geom_boxplot()"
  },
  {
    "objectID": "posts/health_commuting/index.html#mixed-effects-model-summary",
    "href": "posts/health_commuting/index.html#mixed-effects-model-summary",
    "title": "Health and Commute",
    "section": "Mixed Effects Model Summary",
    "text": "Mixed Effects Model Summary\nModel 1: Sickness Absence\n\n\nShow code\nsummary(model1)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: sick_days_PY ~ commute_km_z + gender_dummy + age_z + income_z +  \n    job_change_dummy + full_time_dummy + syear_z + (1 | pid)\n   Data: filtered_data\n\nREML criterion at convergence: 85267.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1702 -0.4957 -0.2456  0.2269  4.6420 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 36.08    6.007   \n Residual             67.61    8.223   \nNumber of obs: 11495, groups:  pid, 6649\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)       9.92801    0.20826  47.671\ncommute_km_z      0.15812    0.10587   1.493\ngender_dummy     -0.31420    0.24473  -1.284\nage_z             0.67980    0.11576   5.872\nincome_z         -1.44511    0.13371 -10.808\njob_change_dummy -0.74317    0.25003  -2.972\nfull_time_dummy   1.49055    0.26091   5.713\nsyear_z           0.50993    0.08545   5.968\n\nCorrelation of Fixed Effects:\n            (Intr) cmmt__ gndr_d age_z  incm_z jb_ch_ fll_t_\ncommut_km_z  0.058                                          \ngender_dmmy -0.312 -0.037                                   \nage_z       -0.097  0.042  0.113                            \nincome_z     0.397 -0.128 -0.169 -0.268                     \njb_chng_dmm -0.219 -0.006  0.023  0.220  0.105              \nfll_tm_dmmy -0.632 -0.049 -0.300 -0.008 -0.398 -0.026       \nsyear_z     -0.044  0.001  0.039 -0.038 -0.104 -0.005  0.027\n\n\nModel 2: Health Status\n\n\nShow code\nsummary(model2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: health_state ~ commute_km_z + gender_dummy + age_z + income_z +  \n    job_change_dummy + full_time_dummy + syear_z + (1 | pid)\n   Data: filtered_data\n\nREML criterion at convergence: 27059.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3911 -0.4941 -0.1345  0.4834  3.6554 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n pid      (Intercept) 0.3363   0.5799  \n Residual             0.3593   0.5994  \nNumber of obs: 11495, groups:  pid, 6649\n\nFixed effects:\n                  Estimate Std. Error t value\n(Intercept)       2.420660   0.017331 139.671\ncommute_km_z     -0.008211   0.008720  -0.942\ngender_dummy      0.002896   0.020652   0.140\nage_z             0.204458   0.009741  20.990\nincome_z         -0.112012   0.011181 -10.018\njob_change_dummy -0.017165   0.019506  -0.880\nfull_time_dummy   0.004147   0.021216   0.195\nsyear_z           0.007406   0.006486   1.142\n\nCorrelation of Fixed Effects:\n            (Intr) cmmt__ gndr_d age_z  incm_z jb_ch_ fll_t_\ncommut_km_z  0.057                                          \ngender_dmmy -0.337 -0.038                                   \nage_z       -0.086  0.040  0.113                            \nincome_z     0.394 -0.123 -0.173 -0.271                     \njb_chng_dmm -0.203 -0.005  0.024  0.204  0.103              \nfll_tm_dmmy -0.617 -0.048 -0.286 -0.011 -0.393 -0.031       \nsyear_z     -0.047  0.002  0.041 -0.057 -0.113  0.003  0.029"
  },
  {
    "objectID": "posts/llm_finetune/index.html",
    "href": "posts/llm_finetune/index.html",
    "title": "Model Fine-tuning",
    "section": "",
    "text": "As part of the Intro to GenAI course by AWS, I completed a project where I fine-tuned the Llama 2 7B large language model for the financial domain. This was done on AWS infrastructure, utilizing tools like Python, Jupyter Notebook, Amazon SageMaker, and S3 bucket for seamless implementation.\nIn this project I fine-tuned the Meta Llama 2 7B large language model in financial domain, deployed the fine-tuned model, and test it’s text generation and domain knowledge capabilities.\nResult comparison table:\n\n\n\n\n\n\n\nBefore fine-tune\nAfter fine-tune\n\n\n\n\n“inputs”: “The investment tests performed indicate”\n“inputs”: “The investment tests performed indicate”\n\n\n“that the proposed method is able to effectively detect the presence of the anomalies in the data. The proposed method is also able to detect anomalies in the data that are not covered by the training data.\nFurthermore, the proposed method is able to detect the anomalies in the data that…”\n“that the majority of the investment opportunities are of low risk, with only 20% of the investments classified as medium risk.\\nThe investment tests performed indicate that the majority of the investment opportunities are of low risk, with only 20% of the investments classified as…”\n\n\n\nBelow is the full code used for this project:\n\n\nFine-tuning model:"
  }
]