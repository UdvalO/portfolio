[
  {
    "objectID": "posts/r_practice/index.html",
    "href": "posts/r_practice/index.html",
    "title": "R Practice",
    "section": "",
    "text": "Through the course “Becoming Fluent in Data and Beyond,” I completed five assignments focused on honing my R skills rather than performing in-depth analysis. These assignments provided hands-on experience with various data types, including time-series, geospatial, panel data, and webscrapping. Here I am sharing some of my favourite plots and analysis showcasing different skills.\n\n\nThis practice provided valuable experience in working with time-series data, converting text data types into time format, and identifying implausible recordings.\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load data\nViaRun_10_05_2023 &lt;- readxl::read_xlsx(\"ViaRun_10_05_2023.xlsx\")\n\n# Convert time string to POSIXct object and extract minutes and seconds (adjusted for the mistake)\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  mutate(time = as.POSIXct(Zeit, format = \"%Y-%m-%d %H:%M:%S\"),\n         Minutes = hour(time),\n         Secs = minute(time))\n\n# Convert Minutes and Secs to a time object\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;% \n  mutate(Time_Fixed = as.POSIXct(paste0(\"00:\", Minutes, \":\", Secs), format = \"%H:%M:%S\"))\n\n# Filter out rows with NA in Time_Fixed column\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  filter(!is.na(Time_Fixed))\n\n# Filter Implausible Records\nimplausible_recording &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &lt;= as.POSIXct(\"00:13:07\", format = \"%H:%M:%S\"))\n\n# Filter everything below 10 minutes\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &gt;= as.POSIXct(\"00:10:00\", format = \"%H:%M:%S\"))\n\n\n\n\nCode\nViaRun_10_05_2023 %&gt;%\n  filter(Gender != \"divers\") %&gt;%\n  group_by(Gender) %&gt;%\n  ggplot(aes(x = Time_Fixed, fill = Gender)) +\n    geom_density(alpha = 0.5, color = \"NA\") +\n    labs(title = \"Density Plot of Time by Gender\",\n       x = \"Time\", y = \"Density\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/r_practice/index.html#various-r-assignments-for-practice",
    "href": "posts/r_practice/index.html#various-r-assignments-for-practice",
    "title": "R Practice",
    "section": "",
    "text": "Through the course “Becoming Fluent in Data and Beyond,” I completed five assignments focused on honing my R skills rather than performing in-depth analysis. These assignments provided hands-on experience with various data types, including time-series, geospatial, panel data, and webscrapping. Here I am sharing some of my favourite plots and analysis showcasing different skills.\n\n\nThis practice provided valuable experience in working with time-series data, converting text data types into time format, and identifying implausible recordings.\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load data\nViaRun_10_05_2023 &lt;- readxl::read_xlsx(\"ViaRun_10_05_2023.xlsx\")\n\n# Convert time string to POSIXct object and extract minutes and seconds (adjusted for the mistake)\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  mutate(time = as.POSIXct(Zeit, format = \"%Y-%m-%d %H:%M:%S\"),\n         Minutes = hour(time),\n         Secs = minute(time))\n\n# Convert Minutes and Secs to a time object\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;% \n  mutate(Time_Fixed = as.POSIXct(paste0(\"00:\", Minutes, \":\", Secs), format = \"%H:%M:%S\"))\n\n# Filter out rows with NA in Time_Fixed column\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n  filter(!is.na(Time_Fixed))\n\n# Filter Implausible Records\nimplausible_recording &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &lt;= as.POSIXct(\"00:13:07\", format = \"%H:%M:%S\"))\n\n# Filter everything below 10 minutes\nViaRun_10_05_2023 &lt;- ViaRun_10_05_2023 %&gt;%\n    filter(Time_Fixed &gt;= as.POSIXct(\"00:10:00\", format = \"%H:%M:%S\"))\n\n\n\n\nCode\nViaRun_10_05_2023 %&gt;%\n  filter(Gender != \"divers\") %&gt;%\n  group_by(Gender) %&gt;%\n  ggplot(aes(x = Time_Fixed, fill = Gender)) +\n    geom_density(alpha = 0.5, color = \"NA\") +\n    labs(title = \"Density Plot of Time by Gender\",\n       x = \"Time\", y = \"Density\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/r_practice/index.html#analysis-of-pokemon-players-in-germany",
    "href": "posts/r_practice/index.html#analysis-of-pokemon-players-in-germany",
    "title": "R Practice",
    "section": "Analysis of Pokemon players in Germany",
    "text": "Analysis of Pokemon players in Germany\nThis was a good practice to work with geospatial data and creating interactive map. Pokemon GO gameplay data was used to plot players’ behavior in Germany including frequency and movement.\n\n\nCode\n# Load libraries\nlibrary(leaflet)\nlibrary(leaflet.extras)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# Load Data\nGameData &lt;- read_delim(\"GameplayLocationHistory.tsv\")\n\n# Rename the columns\nGameData &lt;- GameData %&gt;%\n  rename(\n    lat = `Latitude of location reported by game`,\n    lon = `Longitude of location reported by game`)\n\n# Convert the GameData to a spatial object\ngeodata_sf &lt;- st_as_sf(GameData, coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# Load country boundaries using rnaturalearth\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Filter for Germany\ngermany_boundary &lt;- world %&gt;% \n                    filter(admin == \"Germany\")\n\n# Perform a spatial join to filter points within Germany\ngermany_points &lt;- st_join(geodata_sf, germany_boundary, join = st_within)\n\n\n# Extract lon and lat from geometry column\ngermany_points &lt;- germany_points %&gt;%\n  st_coordinates() %&gt;%\n  as.data.frame() %&gt;%\n  rename(\n    lon = X,\n    lat = Y) %&gt;%\n  bind_cols(germany_points %&gt;% select(-geometry))\n\n# extracting only needed columns\ngermany_points &lt;- germany_points %&gt;% \n  select(1:3)\n\n\n\n\nCode\nlibrary(viridis)\n\n# Calculating start, end date and max date\nstart_date &lt;- min(as.Date(germany_points$`Date and Time`, format = \"%Y-%m-%d\"), na.rm = TRUE)\nend_date &lt;- max(as.Date(germany_points$`Date and Time`, format = \"%Y-%m-%d\"), na.rm = TRUE)\nmax_days &lt;- as.numeric(difftime(end_date, start_date, units = \"days\"))\n\n# Calculating the number of days each location has been recorded\ngermany_points_map &lt;- germany_points %&gt;%\n  mutate(\n    `Date and Time` = as.Date(`Date and Time`),\n    days_recorded = as.numeric(difftime(`Date and Time`, start_date, units = \"days\")))\n\n# Define color palette with viridis\ncolor_palette &lt;- colorNumeric(\n  palette = viridis(7),\n  domain = c(0, 20, 40, 60, 80, 100, 120)\n)\n\n# Create the interactive map\nleaflet(germany_points_map) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(\n    ~lon, ~lat,\n    color = ~color_palette(days_recorded),\n    radius = 3,\n    fillOpacity = 0.8,\n    popup = ~paste(\"Days Recorded:\", days_recorded),\n    label = ~paste0(germany_points$`Date and Time`, \" (Days since start: \", days_recorded, \")\")\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = color_palette,\n    values = ~days_recorded,\n    title = paste(\"Days Since Start:\", \n                  paste(\"Start:\", format(start_date, \"%Y-%m-%d\")),\n                  paste(\"End:\", format(end_date, \"%Y-%m-%d\")),\n                  sep = \"&lt;br&gt;\"),\n    opacity = 1,\n    labFormat = labelFormat(transform = function(x) x)\n  ) %&gt;% \n  \n# Setting the view fixed to Berlin\nsetView(lng = 13.4050, lat = 52.5200, zoom = 6)"
  },
  {
    "objectID": "posts/r_practice/index.html#analysis-of-most-expensive-paintings-in-history",
    "href": "posts/r_practice/index.html#analysis-of-most-expensive-paintings-in-history",
    "title": "R Practice",
    "section": "Analysis of most expensive paintings in history",
    "text": "Analysis of most expensive paintings in history\nHere, I have scrapped a table from a Wikipedia page and cleaned and used the scrapepd to analyze relationships between variables.\n\n\nCode\nlibrary(rvest)\nlibrary(readr)\n\n# URL of the website to scrape\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_most_expensive_paintings#Progression_of_highest_prices_paid\"\n\npage &lt;- read_html(url)\n\ndf_wp &lt;- page %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table(header = TRUE)\n\npaintings &lt;- data.frame(df_wp)\n\n\n\n\nCode\ndf &lt;- read_csv(\"paintings_df.csv\")\n\n# Selecting only needed columns\ndf &lt;- df %&gt;% \n  select(1, 3, 5, 6, 7)\n\n# Renaming columns required\ndf &lt;- df %&gt;% \n  rename(\n    Price = 1,\n    Date = 5\n  )\n\n# Cleaning Price column\ndf &lt;- df %&gt;%\n  mutate(Price_cleaned = Price) %&gt;%  \n  mutate(Price_cleaned = str_replace(Price_cleaned, \"~\", \"\")) %&gt;%\n  mutate(Price_cleaned = str_replace(Price_cleaned, \"\\\\+\", \"\")) %&gt;% \n  mutate(Price_cleaned = str_replace(Price_cleaned, \"\\\\$\", \"\")) %&gt;%\n  mutate(Price_cleaned = str_trim(Price_cleaned)) %&gt;%\n  mutate(Price_cleaned = as.numeric(Price_cleaned))\n\n# taking earleir year for ranged years\ndf &lt;- df %&gt;%\n  mutate(Year_cleaned = case_when(\n    # Handle years with shorthand end dates (e.g., 1904-07)\n    str_detect(Year, \"\\\\d{4}-\\\\d{2}\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # If there's a dash or slash, take the first 4-digit number\n    str_detect(Year, \"-|/\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # If it starts with \"c.\", take the 4-digit number\n    str_starts(Year, \"c\\\\.\") ~ as.numeric(str_extract(Year, \"\\\\d{4}\")),\n    # Otherwise, extract any 4-digit number\n    TRUE ~ as.numeric(str_extract(Year, \"\\\\d{4}\"))\n  ))\n\n# filtering out for final cleaned df\ndf &lt;- df %&gt;%\n  select(-1, -4, -5)\n\n\nSince the Wikipedia page did not provide the artist’s age at the time of painting or at death. I merged another table containing this information to analyze the correlation between the price of the paintings and both the artists’ ages at death and their ages at the time of painting.\n\n\nCode\nplot3 &lt;- df %&gt;%\n  filter(Price_cleaned &gt; 150)\n\n# built based on previous table, containing data above 150m USD\ndf3 &lt;- read_csv(\"above_150.csv\")\n\nplot3$age_at_painting &lt;- df3$Age.at.Painting\nplot3$age_at_death &lt;- df3$Age.at.Death\n\nggplot(plot3, aes(x = age_at_painting, y = Price_cleaned)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.8, se = FALSE, color = \"blue\") +\n  scale_x_continuous(breaks = seq(0, max(plot3$age_at_death), by = 20)) + \n  theme_minimal() +\n  labs(\n    title = \"List of Most Expensive Paintings\",\n    subtitle = \"Paintings over 150 million USD\",\n    x = \"Age at Painting\",\n    y = \"Adjusted Auction Price in Millions USD\"\n  )"
  },
  {
    "objectID": "posts/health_commuting/index.html",
    "href": "posts/health_commuting/index.html",
    "title": "Commuting and Mental Health",
    "section": "",
    "text": "The analysis is in progress and will be published end of August"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Udval Oyunsaikhan",
    "section": "",
    "text": "Hi! I’m Udval. My passion for data analysis was ignited during my time at Meta, where I worked extensively with large datasets. Currently, I am pursuing an MSc in International Business Administration with a specialization in Data Science at the University of Viadrina. This program is further developing my technical expertise and equipping me to drive data-driven enhancements in products, strategies, and operations. My interests lie in urban green mobility, transport data analysis, and trust and safety, with a particular focus on financial fraud and scams.\nFeel free to explore my work from my current studies and get in touch if you have any questions!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thank you for taking the time and showing curiosity to check out my portfolio. Here, you will find the projects I have completed since starting my study in April 2024. I will continue to update this portfolio with new projects. I am also open to collaborating on new projects—if you have a dataset and would like insights, feel free to reach out. Throughout my studies, I have different focus areas each semester:\nSemester 1: R, PowerBI, SQL, and Data Analysis\nSemester 2: Python, Tableau, ML, Statistics, & Business Analytics\nSemester 3: Marketing Analytics, A/B Testing, AI, & ML in Finance\nSemester 4: Currently, I am on the hunt for a research idea\n\nOutside this portfolio\nI hold a MSc in Sustainable Transport Degree from University of Leeds. I have published five articles, done podcast interviews and created and presented a video on public space at the UN Ministerial Summit in 2018.\nI love cooking and eating good food, so during COVID I started my own channel for cooking.\nFacebook | Youtube"
  },
  {
    "objectID": "posts/certificate/index.html",
    "href": "posts/certificate/index.html",
    "title": "Certification Exam",
    "section": "",
    "text": "As part of the certified data analyst exam by DataCamp, I completed a two-hour exam testing my R and SQL skills, followed by a four-hour practical exam where I analyzed sales data from a stationary shop.\nThe goal of the analysis was to determine the best-performing marketing method following the launch of new products. Additionally, I identified high-paying and loyal customers to prioritize them in the marketing strategy for future product launches. Here are some examples but the full report can be found here.\nAnalysis was conduct with following components:\n\nData Cleaning and Validation: first I conducted a validation and cleaning, including correcting categorical inconsistencies, handling missing values, range validation, and ensuring data type consistency.\nExploratory Analysis: here I focused on comprehending the performance of the sales methods over time, specifically examining their impact on sales volume across different states. Additionally, I also analyzed whether loyal customers or new customers showed more interest in the new product offerings.\nKey Business Metric Identification: the key business metric the company continue utilize is sales over time attributed to each method and the comparison of sales in loyal and occasional customers after a product launch.\nRecommendation:\n\nThe revenue experienced exponential growth during the first six weeks. Therefore, continuous monitoring of different sales methods over time is essential to assess their ongoing performance.\nIt is evident that the “Call” method generates the least revenue despite requiring the most employee time. Therefore, it is recommended to significantly reduce the use of this method and focus more on the “Email + Call” and “Email” methods.\nNewer customers tend to purchase more new products, indicating that a marketing plan targeting new customers would be highly beneficial.\nHigh-paying customers (fewer than 500 in total) should be incentivized. Additionally, marketing efforts should target these customers based on their locations to maximize effectiveness.\n\nPresentation: Then the findings were presented through a 12 minute recorded video.\n\n\n\nCode\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nsetwd(\"C:/Users/udwal/Documents/Studies_Viadrina/Becoming Data Fluent/Fluent2024/Bonus Assignment/Portfolio\")\n\n# Importing data\ndata &lt;- read_csv(\"product_sales.csv\")\n\n#  Checking uneque customer IDs\nnum_unique_customers &lt;- length(unique(data$customer_id))\n\n# Checking column unique categorical values \nunique_methods &lt;- unique(data$sales_method)\n\n# fixing typos\ntypo_dict &lt;- c(\n  \"em + call\" = \"Email + Call\",\n  \"email\" = \"Email\")\n\ndata &lt;- data %&gt;%\n  mutate(sales_method = coalesce(typo_dict[sales_method], sales_method))\n\n# Cehcking after fixing typos\nunique_methods &lt;- unique(data$sales_method)\nprint(unique_methods)\n\nunique_product_count &lt;- unique(data$nb_sold)\n\n# Revenue has only 7% missing data and hence droppping NAs\ndata_clean &lt;- data %&gt;% filter(!is.na(revenue))\n\n# values should not exceed 40 and hence filtering\ndata_clean &lt;- data_clean %&gt;% \n    filter(!years_as_customer &gt; 40)\n\n# site visits\nsummary(data_clean$nb_site_visits)\n\nunique_state &lt;- unique(data_clean$state)\nprint(unique_state)\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\npastel_colors &lt;- brewer.pal(6, \"Pastel2\")\n\nggplot(data_clean, aes(x = as.factor(week), y = revenue, fill = sales_method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = pastel_colors) +\n  labs(x = \"Week\", y = \"Revenue\", fill = \"Sales Method\") +\n  ggtitle(\"Revenue by Week and Sales Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data_clean, aes(y = revenue)) +\n  geom_boxplot() +\n  labs(y = \"Revenue\") +\n  ggtitle(\"Revenue Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter high-revenue customers (above 200.00)\nhigh_rev_cust &lt;- data_clean %&gt;%\n  filter(revenue &gt; 200.00)\n\n# Aggregate data by state\ndata_aggregated &lt;- high_rev_cust %&gt;%\n  group_by(state) %&gt;%\n  summarise(total_revenue = sum(revenue, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Sort data by total_revenue in descending order\ndata_aggregated &lt;- data_aggregated %&gt;%\n  arrange(desc(total_revenue))\n\n# Reorder state factor levels based on total_revenue\ndata_aggregated$state &lt;- factor(data_aggregated$state, levels = data_aggregated$state)\n\nggplot(data_aggregated, aes(x = \"\", y = state, fill = total_revenue)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"orange\", name = \"Total Revenue\") +\n  labs(x = \"State\", y = NULL, fill = \"Total Revenue\") +\n  ggtitle(\"Total Revenue by State for High Value Customers\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 6)\n  )"
  },
  {
    "objectID": "posts/certificate/index.html#certified-data-analyst-exam-submission",
    "href": "posts/certificate/index.html#certified-data-analyst-exam-submission",
    "title": "Certification Exam",
    "section": "",
    "text": "As part of the certified data analyst exam by DataCamp, I completed a two-hour exam testing my R and SQL skills, followed by a four-hour practical exam where I analyzed sales data from a stationary shop.\nThe goal of the analysis was to determine the best-performing marketing method following the launch of new products. Additionally, I identified high-paying and loyal customers to prioritize them in the marketing strategy for future product launches. Here are some examples but the full report can be found here.\nAnalysis was conduct with following components:\n\nData Cleaning and Validation: first I conducted a validation and cleaning, including correcting categorical inconsistencies, handling missing values, range validation, and ensuring data type consistency.\nExploratory Analysis: here I focused on comprehending the performance of the sales methods over time, specifically examining their impact on sales volume across different states. Additionally, I also analyzed whether loyal customers or new customers showed more interest in the new product offerings.\nKey Business Metric Identification: the key business metric the company continue utilize is sales over time attributed to each method and the comparison of sales in loyal and occasional customers after a product launch.\nRecommendation:\n\nThe revenue experienced exponential growth during the first six weeks. Therefore, continuous monitoring of different sales methods over time is essential to assess their ongoing performance.\nIt is evident that the “Call” method generates the least revenue despite requiring the most employee time. Therefore, it is recommended to significantly reduce the use of this method and focus more on the “Email + Call” and “Email” methods.\nNewer customers tend to purchase more new products, indicating that a marketing plan targeting new customers would be highly beneficial.\nHigh-paying customers (fewer than 500 in total) should be incentivized. Additionally, marketing efforts should target these customers based on their locations to maximize effectiveness.\n\nPresentation: Then the findings were presented through a 12 minute recorded video.\n\n\n\nCode\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nsetwd(\"C:/Users/udwal/Documents/Studies_Viadrina/Becoming Data Fluent/Fluent2024/Bonus Assignment/Portfolio\")\n\n# Importing data\ndata &lt;- read_csv(\"product_sales.csv\")\n\n#  Checking uneque customer IDs\nnum_unique_customers &lt;- length(unique(data$customer_id))\n\n# Checking column unique categorical values \nunique_methods &lt;- unique(data$sales_method)\n\n# fixing typos\ntypo_dict &lt;- c(\n  \"em + call\" = \"Email + Call\",\n  \"email\" = \"Email\")\n\ndata &lt;- data %&gt;%\n  mutate(sales_method = coalesce(typo_dict[sales_method], sales_method))\n\n# Cehcking after fixing typos\nunique_methods &lt;- unique(data$sales_method)\nprint(unique_methods)\n\nunique_product_count &lt;- unique(data$nb_sold)\n\n# Revenue has only 7% missing data and hence droppping NAs\ndata_clean &lt;- data %&gt;% filter(!is.na(revenue))\n\n# values should not exceed 40 and hence filtering\ndata_clean &lt;- data_clean %&gt;% \n    filter(!years_as_customer &gt; 40)\n\n# site visits\nsummary(data_clean$nb_site_visits)\n\nunique_state &lt;- unique(data_clean$state)\nprint(unique_state)\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\npastel_colors &lt;- brewer.pal(6, \"Pastel2\")\n\nggplot(data_clean, aes(x = as.factor(week), y = revenue, fill = sales_method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = pastel_colors) +\n  labs(x = \"Week\", y = \"Revenue\", fill = \"Sales Method\") +\n  ggtitle(\"Revenue by Week and Sales Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data_clean, aes(y = revenue)) +\n  geom_boxplot() +\n  labs(y = \"Revenue\") +\n  ggtitle(\"Revenue Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter high-revenue customers (above 200.00)\nhigh_rev_cust &lt;- data_clean %&gt;%\n  filter(revenue &gt; 200.00)\n\n# Aggregate data by state\ndata_aggregated &lt;- high_rev_cust %&gt;%\n  group_by(state) %&gt;%\n  summarise(total_revenue = sum(revenue, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Sort data by total_revenue in descending order\ndata_aggregated &lt;- data_aggregated %&gt;%\n  arrange(desc(total_revenue))\n\n# Reorder state factor levels based on total_revenue\ndata_aggregated$state &lt;- factor(data_aggregated$state, levels = data_aggregated$state)\n\nggplot(data_aggregated, aes(x = \"\", y = state, fill = total_revenue)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"orange\", name = \"Total Revenue\") +\n  labs(x = \"State\", y = NULL, fill = \"Total Revenue\") +\n  ggtitle(\"Total Revenue by State for High Value Customers\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 6)\n  )"
  },
  {
    "objectID": "posts/powerbi/index.html",
    "href": "posts/powerbi/index.html",
    "title": "Sales Data Analysis",
    "section": "",
    "text": "As part of the Data Analysis course, I conducted an analysis of sales data from a bike shop active in three states of Australia using PowerBI. Take a look at the raw data…\nTo enhance the validity of my findings and uncover deeper insights, I also explored external cycling trends by analyzing the National Annual Cycling Survey data from Australia. This analysis was performed using R, allowing me to cross-reference sales data with broader industry trends.\n\n\n\n\n\n\nThe project information at glance:\n\nBelow are the detailed steps of the analysis:\n\nBusiness Question: The analysis began with defining the business questions and objectives based on the availability of the dataset and its variables.\nData Cleaning and Modeling: The raw data presented several issues that required a degree of cleaning and preparation. To address these, I used DAX queries for data transformation and modeling.\nVisualization: Using PowerBI, I created four dashboards to summarize the analysis comprehensively. These dashboards focused on identifying target customer segments, uncovering patterns in their purchasing behavior, and identifying high-value and loyal customers. Then I looked into the sales performance, including profit margins for popular products and seasonal trends. This approach allowed for a detailed view of the data, facilitating deeper insights into both customer dynamics and sales performance.\nExternal Trend: Using R, I selected key variables from the dataset, prepared the data, and analyzed external trends in cycling.\nReport: Finally, I compiled the findings and recommendations into a comprehensive report. Full report is available on request."
  },
  {
    "objectID": "posts/powerbi/index.html#analysis-of-bike-sales-data-in-australia",
    "href": "posts/powerbi/index.html#analysis-of-bike-sales-data-in-australia",
    "title": "Sales Data Analysis",
    "section": "",
    "text": "As part of the Data Analysis course, I conducted an analysis of sales data from a bike shop active in three states of Australia using PowerBI. Take a look at the raw data…\nTo enhance the validity of my findings and uncover deeper insights, I also explored external cycling trends by analyzing the National Annual Cycling Survey data from Australia. This analysis was performed using R, allowing me to cross-reference sales data with broader industry trends.\n\n\n\n\n\n\nThe project information at glance:\n\nBelow are the detailed steps of the analysis:\n\nBusiness Question: The analysis began with defining the business questions and objectives based on the availability of the dataset and its variables.\nData Cleaning and Modeling: The raw data presented several issues that required a degree of cleaning and preparation. To address these, I used DAX queries for data transformation and modeling.\nVisualization: Using PowerBI, I created four dashboards to summarize the analysis comprehensively. These dashboards focused on identifying target customer segments, uncovering patterns in their purchasing behavior, and identifying high-value and loyal customers. Then I looked into the sales performance, including profit margins for popular products and seasonal trends. This approach allowed for a detailed view of the data, facilitating deeper insights into both customer dynamics and sales performance.\nExternal Trend: Using R, I selected key variables from the dataset, prepared the data, and analyzed external trends in cycling.\nReport: Finally, I compiled the findings and recommendations into a comprehensive report. Full report is available on request."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Sales Data Analysis\n\n\n\n\n\n\nPowerBI\n\n\nR\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR Practice\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCertification Exam\n\n\n\n\n\n\nR\n\n\nSQL\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCommuting and Mental Health\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\nupcoming\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\nNo matching items"
  }
]